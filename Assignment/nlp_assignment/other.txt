i am making a classifier that produces a polarity label: positive, negative or neutral. use pytorch and the gpu.

Each line contains 5 tab-separated fields: the polarity of the opinion (the ground truth polarity label), the aspect category on which the opinion is expressed, a specific target term, the character offsets of the term (start:end), and the sentence in which the term occurs and the opinion is expressed.

example:
positive	AMBIENCE#GENERAL	seating	18:25	short and sweet â€“ seating is great:it's romantic,cozy and private.
positive	AMBIENCE#GENERAL	trattoria	25:34	This quaint and romantic trattoria is at the top of my Manhattan restaurant list.
positive	FOOD#QUALITY	food	98:102	The have over 100 different beers to offer thier guest so that made my husband very happy and the food was delicious, if I must recommend a dish it must be the pumkin tortelini.
negative	SERVICE#GENERAL	STAFF	5:10	THIS STAFF SHOULD BE FIRED.

There are 12 different aspects categories:
AMBIENCE#GENERAL
DRINKS#PRICES
DRINKS#QUALITY
DRINKS#STYLE_OPTIONS
FOOD#PRICES
FOOD#QUALITY
FOOD#STYLE_OPTIONS
LOCATION#GENERAL
RESTAURANT#GENERAL
RESTAURANT#MISCELLANEOUS
RESTAURANT#PRICES
SERVICE#GENERAL

the data has target proportions:
positive    0.701333
negative    0.261333
neutral     0.037333

Here is my code, can you check it to see if parts can be improved/corrected:


# 1. Load the data

train_path = "data/traindata.csv"
val_path = "data/devdata.csv"

df_train = pd.read_csv(train_path, sep='\t', header=0, index_col=False)
df_val = pd.read_csv(val_path, sep='\t', header=0, index_col=False)

# 2. Make the datasets

X_train = df_train.iloc[:, 1:]
y_train = df_train.iloc[:, 0]
print(X_train.shape , y_train.shape) >>> (1502, 4) (1502,)

X_val = df_val.iloc[:, 1:]
y_val = df_val.iloc[:, 0]
print(X_val.shape , y_val.shape) >>> (375, 4) (375,)

print(y_train.value_counts(normalize=True))
print(y_val.value_counts(normalize=True))

# Train
aspect_encoder = LabelEncoder()
polarity_encoder = LabelEncoder()
df_train['aspect_category_encoded'] = aspect_encoder.fit_transform(df_train.iloc[:, 1])
y_train_encoded = polarity_encoder.fit_transform(y_train)
X_train_sentences = df_train.iloc[:, -1] # sentence is the last column
print(f"Encoded labels shape: {y_train_encoded.shape}")
print(f"Input sentences shape: {X_train_sentences.shape}")

# Validation
aspect_encoder_val = LabelEncoder()
polarity_encoder_val = LabelEncoder()
df_val['aspect_category_encoded'] = aspect_encoder_val.fit_transform(df_val.iloc[:, 1])
y_val_encoded = polarity_encoder_val.fit_transform(y_val)
X_val_sentences = df_val.iloc[:, -1]
print(f"Encoded labels shape: {y_val_encoded.shape}")
print(f"Input sentences shape: {X_val_sentences.shape}")

# 3. Dataset definition

class ReviewDataset(Dataset):
    def __init__(self, sentences, labels, tokenizer, max_len=128):
        self.sentences = sentences
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        
    def __len__(self):
        return len(self.sentences)
    
    def __getitem__(self, idx):
        sentence = str(self.sentences.iloc[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer.encode_plus(
          sentence,
          add_special_tokens=True,
          max_length=self.max_len,
          return_token_type_ids=False,
          padding='max_length',
          return_attention_mask=True,
          return_tensors='pt',
        )
        
        return {
          'review_text': sentence,
          'input_ids': encoding['input_ids'].flatten(),
          'attention_mask': encoding['attention_mask'].flatten(),
          'labels': torch.tensor(label, dtype=torch.long)
        }

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

batch_size = 32

dataset = ReviewDataset(X_train_sentences, y_train_encoded, tokenizer)
loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

val_dataset = ReviewDataset(X_val_sentences, y_val_encoded, tokenizer)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)

# 4. Model definition

class SentimentClassifier(nn.Module):
    def __init__(self, n_classes):
        super(SentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.drop1 = nn.Dropout(p=0.3)  # First dropout layer
        # Adding a fully connected layer to introduce more capacity to the model
        self.fc1 = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size // 2)
        self.drop2 = nn.Dropout(p=0.2)  # Second dropout layer
        # Layer normalization
        self.layer_norm = nn.LayerNorm(self.bert.config.hidden_size // 2)
        # Final output layer
        self.out = nn.Linear(self.bert.config.hidden_size // 2, n_classes)
    
    def forward(self, input_ids, attention_mask):
        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)
        dropped_output = self.drop1(pooled_output)
        # Applying the fully connected layer after the first dropout
        fc1_output = F.relu(self.fc1(dropped_output))
        dropped_output = self.drop2(fc1_output)
        # Applying layer normalization
        norm_output = self.layer_norm(dropped_output)
        return self.out(norm_output)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SentimentClassifier(n_classes=len(polarity_encoder.classes_)).to(device)

optimizer = Adam(model.parameters(), lr=2e-5)

class_proportions = {0: 0.26, 1: 0.04, 2: 0.7}
class_weights = {class_label: (1.0 / proportion) for class_label, proportion in class_proportions.items()}
weight_sum = sum(class_weights.values())
num_classes = len(class_weights)
class_weights = {class_label: (weight / weight_sum) * num_classes for class_label, weight in class_weights.items()}
weights_tensor = torch.tensor(list(class_weights.values()), dtype=torch.float32)
loss_fn = nn.CrossEntropyLoss(weight=weights_tensor).to(device)

# 5. Train loop

def train_epoch(model, data_loader, loss_fn, optimizer, device, n_examples):
    model = model.train()
    losses = []
    correct_predictions = 0
    
    for d in data_loader:
        input_ids = d["input_ids"].to(device)
        attention_mask = d["attention_mask"].to(device)
        labels = d["labels"].to(device)
        
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        
        _, preds = torch.max(outputs, dim=1)
        loss = loss_fn(outputs, labels)
        
        correct_predictions += torch.sum(preds == labels)
        losses.append(loss.item())
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    return correct_predictions.double() / n_examples, np.mean(losses)

# 6. Evaluation loop

from sklearn.metrics import precision_score, recall_score, f1_score

def eval_model(model, data_loader, loss_fn, device, n_examples):
    model = model.eval()
    losses = []
    correct_predictions = 0
    predictions = []
    true_labels = []
    
    with torch.no_grad():
        for d in data_loader:
            input_ids = d["input_ids"].to(device)
            attention_mask = d["attention_mask"].to(device)
            labels = d["labels"].to(device)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            _, preds = torch.max(outputs, dim=1)
            
            loss = loss_fn(outputs, labels)
            
            correct_predictions += torch.sum(preds == labels)
            losses.append(loss.item())

            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
    
    # metrics
    precision = precision_score(true_labels, predictions, average='weighted', zero_division=0)
    recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)
    f1 = f1_score(true_labels, predictions, average='weighted', zero_division=0)

    return correct_predictions.double() / n_examples, np.mean(losses), precision, recall, f1

# 7. Training

def train_model(model, train_data_loader, val_data_loader, loss_fn, optimizer, device, epochs=4):
    for epoch in range(epochs):
        print(f'Epoch {epoch + 1}/{epochs}')
        print('-' * 10)

        train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device, len(train_data_loader.dataset))
        print(f'Train loss {train_loss} accuracy {train_acc}')

        val_acc, val_loss, val_precision, val_recall, val_f1 = eval_model(model, val_data_loader, loss_fn, device, len(val_data_loader.dataset))
        print(f'Val loss {val_loss} accuracy {val_acc}')
        print(f'Precision: {val_precision}, Recall: {val_recall}, F1-score: {val_f1}\n')

train_model(model, loader, val_loader, loss_fn, optimizer, 'cuda', epochs=10)

# 8. Predictions

def eval_model_with_predictions(model, data_loader, device):
    
    model.eval()
    predictions = []
    true_labels = []

    with torch.no_grad():
        for d in data_loader:
            input_ids = d["input_ids"].to(device)
            attention_mask = d["attention_mask"].to(device)
            labels = d["labels"].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            _, preds = torch.max(outputs, dim=1)


            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())

    return predictions, true_labels

predictions, true_labels = eval_model_with_predictions(model, val_loader, device)

# 9. Results

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(true_labels, predictions)

fig, ax = plt.subplots(figsize=(8, 8))
sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap="Blues", square=True,
            xticklabels=['Predicted Negative', 'Predicted Neutral', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Neutral', 'Actual Positive'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()